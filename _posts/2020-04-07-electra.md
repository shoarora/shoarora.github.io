---
title: ELECTRA
tags:
 - pytorch
 - nlp
---

A couple weeks ago, I released a couple models in [`huggingface/transformers`](https://github.com/huggingface/transformers).

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Just put up two new models in <a href="https://twitter.com/huggingface?ref_src=twsrc%5Etfw">@huggingface</a> transformers!<br><br>One is a Bert-based unofficial implementation of ELECTRA-small, and the other is an Albert model (calling it ALECTRA, very creative) created with the same pre-training task <br><br>1/4 <a href="https://t.co/yuwJQySLYv">pic.twitter.com/yuwJQySLYv</a></p>&mdash; Sho Arora (@shoarora7) <a href="https://twitter.com/shoarora7/status/1244539489218752513?ref_src=twsrc%5Etfw">March 30, 2020</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

They're based on [ELECTRA](https://openreview.net/forum?id=r1xMH1BtvB), a transformer model with a new pretraining task from Stanford NLP / Google Research.
I wrote about the specific experiments that produced these models [here](https://github.com/shoarora/lmtuners/tree/master/experiments/disc_lm_small).
To close the loop, I thought I'd write a little about the task itself and break down the code as needed.

## Task
![Task Diagram](https://github.com/shoarora/lmtuners/raw/master/assets/electra.png)
(figure from [Clark et al. 2020](https://openreview.net/pdf?id=r1xMH1BtvB))

The ELECTRA pre-training task in short, simultaneously trains a generator to replace tokens in text, and a discriminator that detects original/replaced tokens.

The act of replacing tokens is done using a [Masked Languge Model](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270).

I used the [`huggingface/transformers`](https://github.com/huggingface/transformers) implemenation of various models.  In particular, they have
`BertForMaskedLM` and `BertForTokenClassification` models, which are exactly what the generator and discriminator need to do.  In fact,
they have other transformer models that use the same interface, so the pre-training task generalizes to other transformer models.  This made it easy for
me to train an `ALECTRA` model.

### Code
I'll break down the [source code](https://github.com/shoarora/lmtuners/blob/master/lmtuners/lightning_modules/discriminative_lm.py#L57)
 step by step, with some edits made for clarity, and may not be performant.

```python
def forward(self, inputs, labels, attention_mask, token_type_ids):
    # copy the variables for use with discriminator.
    d_inputs = inputs.clone()

    # run masked LM.
    g_out = self.generator(inputs,
                            masked_lm_labels=labels,
                            attention_mask=attention_mask,
                            token_type_ids=token_type_ids)
```
The forward pass of the pre-training task begins just by running the masked LM.  The key step is to sample tokens from these predictions to pass to the generator.

```python
# get samples from masked LM.
# when masked_lm_labels is passed into the generator forward function, the value at index 1 is the output logits.
sample_probs = torch.softmax(g_out[1], dim=-1, dtype=torch.float32)  # [batch_size, seq_len, vocab_size]
sample_probs = sample_probs.view(-1, self.vocab_size)                # [batch_size * seq_len, vocab_size]

sampled_tokens = torch.multinomial(sample_probs, 1).view(-1)         # [batch_size * seq_len,]
sampled_tokens = sampled_tokens.view(d_inputs.shape[0], -1)          # [batch_size, seq_len]
```
We first apply a softmax to get token probabilities.  We need to then reshape the tensor into 2d to sample with `torch.multinomial`.
Here, `torch.multinomial(x, n)` picks `n` indices per slice along the first dimension of `x`, according to the probability distribution
described in the second dimension of `x`.

```python
x = [[0.4, 0.1, 0.2, 0.3],
     [0.8, 0.05, 0.05, 0.05]]
```
In this example, each row represents a probability distribution over all the possible tokens.  We sample one index per row based on these probabilities.
Naturally, this yields a different result than just taking the `argmax` over the probabilities, and is important in reaching target performance.
The [original paper](https://openreview.net/forum?id=r1xMH1BtvB) studies the effect of different sampling techniques.

Next, we use the sampled token indices to construct the discriminator input:
```python
d_inputs = inputs.clone()

# labels have a -100 value to mask out loss from unchanged tokens.
masked_indices = labels.ne(-100)

# replace the masked out tokens of the input with the generator predictions.
d_inputs[masked_indices] = sampled_tokens[masked_indices]
```

It's possible that the replaced token turns out to be the correct token.  The authors report moderate improvement by marking such tokens as original.
So that's how we'll construct our labels:
```python
# turn mask into new target labels.  1 (True) for corrupted, 0 for original.
# if the prediction was correct, mark it as uncorrupted.
correct_preds = sampled_tokens == labels
d_labels = masked_indices.long()
d_labels[correct_preds] = 0
```

Finally we run the discriminator forward pass and compute the loss.
```python
# run token classification, predict whether each token was corrupted.
d_out = self.discriminator(d_inputs,
                            labels=d_labels,
                            attention_mask=attention_mask,
                            token_type_ids=token_type_ids)

# losses are placed in the 0-index of the output tuples.
loss = g_out[0] + d_loss_weight * d_out[0]
```
We need another hyperparameter `d_loss_weight` to scale up the discriminator loss into the same range as the generator loss.

## Model

The original authors used a relatively standard Bert model for both the generator and discriminator with a couple tweaks:
 - `embedding_size != hidden_size`: they used a smaller embedding sized and projected it up to the hidden size, similar to [Albert](https://arxiv.org/abs/1909.11942).
 - They shared the embedding weights between the generator and discriminator.
 - They used the embedding matrix as the decoding matrix (Bert from `huggingface` didn't do this by default).

The original paper found sharing embedding weights to be largely beneficial, but found it more efficient to use
a smaller generator rather than tie the encoder layer weights themselves.

![electra generator sizes](/assets/images/electra-generator-sizes.png)
(figure from [Clark et al. 2020](https://openreview.net/pdf?id=r1xMH1BtvB))

## Conclusions & Takeaways

I learned a lot and had some pitfalls trying to get this off the ground.  I tried to get this code running on TPU with `pytorch-lightning`, but this required a few modifications to my implementation.
Namely, numpy-like tensor indexing is not performant.  I had to change them all to `torch.where` statements.
I also should've run the token sampling portion for only the masked indices.  On GPU, this turned out to make a negligible difference
in performance.  Ultimately, the `LAMB` optimizer implementation also seemed to be slow on TPU and I decided to forego this for now.

My biggest non-modelling error was in data preparation.
I had only one text segment per data point, which I thought was fine since ELECTRA didn't use the Next-Sentence-Prediction task that Bert did.
However, I was using the same evaluation task as Bert, which required passing in two text segments at once.  Performance on the evaluation
tasks varied greatly by this change.

I used the `GLUE` tasks for model evaluation.  I found it took a lot of effort and resources to match reported results.  Some tasks have multiple
evaluation metrics, and some papers report differing metrics (or don't even specify what metric they reported).  They also sometimes use ensembling/multi-model
techniques to max out their scores.  Some authors perform intensive hyperparameter searches to maximize their scores.  Moreover, variance in fine-tuning can be [quite large](https://arxiv.org/abs/2002.06305).  I ultimately didn't worry about replicating the scores right on the dot, and settled for anything close.

`huggingface/transformers` has a [script](https://github.com/huggingface/transformers/blob/master/examples/run_glue.py) for running the `GLUE` tasks.  It didn't produce test set predictions though, and the ELECTRA paper didn't report
task-by-task dev scores.  So I made a [contribution](https://github.com/huggingface/transformers/pull/3405) to the repo to do exactly this.  Contributing back
to the library was a bonus experience.

I originally began this project trying to create an end-to-end implementation with easy to run scripts.  For reproducing ELECTRA, I tried to do exactly that.
I also did my best to break out the task implementation from the specific models used so that this work can be extended upon.  Hopefully, I did enough
to do so and showed this by producing an Albert based model.  The source code is still WIP, but I'm pleased that I was able to produce results.
